{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review \n",
    "우리는 라우터를 구축했습니다.\n",
    "- 우리의 채팅 모델은 사용자 입력에 따라 도구를 호출할지 말지를 결정합니다.\n",
    "- 조건부 엣지를 사용하여 도구를 호출할 노드로 라우팅하거나 단순히 종료합니다.\n",
    "\n",
    "![Screenshot 2024-08-21 at 12.44.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0ba0bd34b541c448cc_agent1.png)\n",
    "\n",
    "\n",
    "## Goals\n",
    "이제 이를 일반적인 에이전트 아키텍처로 확장할 수 있습니다.\n",
    "\n",
    "위의 라우터에서는 모델을 호출하고, 모델이 도구 호출을 선택하면 사용자에게 ToolMessage를 반환했습니다.\n",
    "\n",
    "하지만, 만약 그 ToolMessage를 단순히 모델에게 다시 전달한다면 어떨까요?\n",
    "\n",
    "모델이 (1) 다른 도구를 호출하거나 (2) 직접 응답하도록 할 수 있습니다.\n",
    "\n",
    "이것이 ReAct라는 일반 에이전트 아키텍처의 직관입니다.\n",
    "\n",
    "* `act` - 모델이 특정 도구를 호출하도록 합니다.\n",
    "* `observe` - 도구의 출력을 모델에게 다시 전달합니다.\n",
    "* `reason` - 모델이 도구 출력에 대해 이유를 분석하여 다음에 무엇을 할지 결정하도록 합니다 (예: 다른 도구를 호출하거나 직접 응답).\n",
    "이와 같은 [범용 아키텍처](https://blog.langchain.dev/planning-for-agents/)는 다양한 종류의 도구에 적용될 수 있습니다.\n",
    "\n",
    "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    print(f\"Multiplying {a} and {b}\")\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    print(f\"Adding {a} and {b}\")\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    print(f\"Dividing {a} by {b}\")\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# 이 ipynb에서는 수학 연산이 일반적으로 순차적으로 수행되기 때문에 parallel tool calling을 false로 설정했습니다.\n",
    "# 이번에는 수학 연산을 수행할 수 있는 3개의 도구가 있습니다.\n",
    "# 참고로, OpenAI 모델은 효율성을 위해 기본적으로 병렬 도구 호출을 사용합니다. 자세한 내용은 https://python.langchain.com/docs/how_to/tool_calling_parallel/ 을 참고하세요.\n",
    "# 다양한 방식으로 실험해 보시고 모델이 수학 방정식에 대해 어떻게 동작하는지 확인해보세요!\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 같이, 우리는 `MessagesState`를 사용하고 도구 목록을 가진 Tools 노드를 정의합니다.\n",
    "\n",
    "`Assistant` 노드는 바인딩된 도구와 함께 작동하는 모델입니다.\n",
    "\n",
    "`Assistant`와 `Tools` 노드로 구성된 그래프를 생성합니다.\n",
    "\n",
    "그리고 `tools_condition` 엣지를 추가하여, `Assistant`가 도구를 호출하는지에 따라 `End` 또는 `Tools`로 라우팅합니다.\n",
    "\n",
    "이제 한 가지 새로운 단계를 추가합니다:\n",
    "\n",
    "`Tools` 노드를 `Assistant`에 다시 연결하여 루프를 형성합니다.\n",
    "\n",
    "`Assistant` 노드가 실행된 후, `tools_condition`은 모델의 출력이 도구 호출인지 확인합니다.\n",
    "만약 도구 호출이면, 흐름은 `Tools` 노드로 이동합니다.\n",
    "그리고 `Tools` 노드는 다시 `Assistant`에 연결됩니다.\n",
    "\n",
    "이 루프는 모델이 계속 도구를 호출하기로 결정하는 한 반복됩니다.\n",
    "만약 모델의 응답이 도구 호출이 아니라면, 흐름은 END로 이동하여 프로세스를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tools_condition' from 'langgraph.prebuilt' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m START, StateGraph\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tools_condition\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolNode\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'tools_condition' from 'langgraph.prebuilt' (unknown location)"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
